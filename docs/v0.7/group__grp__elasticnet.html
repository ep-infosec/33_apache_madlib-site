<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>MADlib: Elastic Net Regularization</title>

<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script src="../mathjax/MathJax.js">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
</head>
<body>
<div id="top"><!-- do not remove this div! -->


<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  
  
  <td style="padding-left: 0.5em;">
   <div id="projectname">MADlib
   &#160;<span id="projectnumber">0.7</span> <span style="font-size:10pt; font-style:italic"><a href="../latest/./group__grp__elasticnet.html"> A newer version is available</a></span>
   </div>
   <div id="projectbrief">User Documentation</div>
  </td>
  
  
  
 </tr>
 </tbody>
</table>
</div>

<!-- Generated by Doxygen 1.7.5.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="dynsections.js"></script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div>
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
  initNavTree('group__grp__elasticnet.html','');
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Elastic Net Regularization</div>  </div>
<div class="ingroups"><a class="el" href="group__grp__suplearn.html">Supervised Learning</a></div></div>
<div class="contents">
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Collaboration diagram for Elastic Net Regularization:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<center><table><tr><td><div class="center"><iframe scrolling="no" frameborder="0" src="group__grp__elasticnet.svg" width="395" height="40"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</td></tr></table></center>
</div>
<dl class="user"><dt><b>About:</b></dt><dd></dd></dl>
<p>This module implements the elastic net regularization for regression problems.</p>
<p>This method seeks to find a weight vector that, for any given training example set, minimizes: </p>
<p class="formulaDsp">
\[\min_{w \in R^N} L(w) + \lambda \left(\frac{(1-\alpha)}{2} \|w\|_2^2 + \alpha \|w\|_1 \right)\]
</p>
<p> where \(L\) is the metric function that the user wants to minimize. Here \( \alpha \in [0,1] \) and \( lambda \geq 0 \). If \(alpha = 0\), we have the ridge regularization (known also as Tikhonov regularization), and if \(\alpha = 1\), we have the LASSO regularization.</p>
<p>For the Gaussian response family (or linear model), we have </p>
<p class="formulaDsp">
\[L(\vec{w}) = \frac{1}{2}\left[\frac{1}{M} \sum_{m=1}^M (w^{t} x_m + w_{0} - y_m)^2 \right] \]
</p>
<p>For the Binomial response family (or logistic model), we have </p>
<p class="formulaDsp">
\[ L(\vec{w}) = \sum_{m=1}^M\left[y_m \log\left(1 + e^{-(w_0 + \vec{w}\cdot\vec{x}_m)}\right) + (1-y_m) \log\left(1 + e^{w_0 + \vec{w}\cdot\vec{x}_m}\right)\right]\ , \]
</p>
<p> where \(y_m \in {0,1}\).</p>
<p>To get better convergence, one can rescale the value of each element of x </p>
<p class="formulaDsp">
\[ x&#39; \leftarrow \frac{x - \bar{x}}{\sigma_x} \]
</p>
<p> and for Gaussian case we also let </p>
<p class="formulaDsp">
\[y&#39; \leftarrow y - \bar{y} \]
</p>
<p> and then minimize with the regularization terms. At the end of the calculation, the orginal scales will be restored and an intercept term will be obtained at the same time as a by-product.</p>
<p>Note that fitting after scaling is not equivalent to directly fitting.</p>
<p>Right now, two optimizers are supported. The default one is FISTA, and the other is IGD. They have their own parameters, which can be specified in the <em>optimizer_params</em> as a text array. For example, 'max_stepsize = 0.1, warmup = t, warmup_lambdas = [0.4, 0.3, 0.2]'.</p>
<p><b>(1) FISTA</b></p>
<p>Fast Iterative Shrinkage Thresholding Algorithm (FISTA) [2] has the following optimizer-specific parameters:</p>
<p>max_stepsize - default is 4.0 eta - default is 2, if stepsize does not work stepsize/eta will be tried warmup - default is False warmup_lambdas - default is NULL, which means that lambda values will be automatically generated warmup_lambda_no - default is 15. How many lambda's are used in warm-up, will be overridden if warmup_lambdas is not NULL warmup_tolerance - default is the same as tolerance. The value of tolerance used during warmup. use_active_set - default is False. Whether to use active-set method to speed up the computation. activeset_tolerance - default is the same as tolerance. The value of tolerance used during active set calculation random_stepsize - default is False. Whether add some randomness to the step size. Sometimes, this can speed up the calculation.</p>
<p>Here, backtracking for step size is used. At each iteration, we first try the <em>stepsize = max_stepsize</em>, and if it does not work out, we then try a smaller step size <em>stepsize = stepsize / eta</em>, where <em>eta</em> must be larger than 1. At first sight, this seems to do repeated iterations for even one step, but it actually greatly increases the computation speed by using a larger step size and minimizes the total number of iterations. A careful choice of max_stepsize can decrease the computation time by more than 10 times.</p>
<p>If <em>warmup</em> is <em>True</em>, a series of lambda values, which is strictly descent and ends at the lambda value that the user wants to calculate, will be used. The larger lambda gives very sparse solution, and the sparse solution again is used as the initial guess for the next lambda's solution, which will speed up the computation for the next lambda. For larger data sets, this can sometimes accelerate the whole computation and might be faster than computation on only one lambda value.</p>
<p>If <em>use_active_set</em> is <em>True</em>, active-set method will be used to speed up the computation. Considerable speedup is obtained by organizing the iterations around the active set of featuresâ€” those with nonzero coefficients. After a complete cycle through all the variables, we iterate on only the active set till convergence. If another complete cycle does not change the active set, we are done, otherwise the process is repeated.</p>
<p><b>(2) IGD</b></p>
<p>Incremental Gradient Descent (IGD) or Stochastic Gradient Descent (SGD) [3] has the following optimizer-specific parameters:</p>
<p>stepsize - default is 0.01 threshold - default is 1e-10. When a coefficient is really small, set it to be 0 warmup - default is False warmup_lambdas - default is Null warmup_lambda_no - default is 15. How many lambda's are used in warm-up, will be overridden if warmup_lambdas is not NULL warmup_tolerance - default is the same as tolerance. The value of tolerance used during warmup. parallel - default is True. Run the computation on multiple segments or not.</p>
<p>Due to the stochastic nature of SGD, we can only obtain very small values for the fitting coefficients. Therefore, <em>threshold</em> is needed at the end of the computation to screen out those tiny values and just hard set them to be zeros. This is done as the following: (1) multiply each coefficient with the standard deviation of the corresponding feature (2) compute the average of absolute values of re-scaled coefficients (3) divide each rescaled coefficients with the average, and if the resulting absolute value is smaller than <em>threshold</em>, set the original coefficient to be zero.</p>
<p>SGD is in nature a sequential algorithm, and when running in a distributed way, each segment of the data runs its own SGD model, and the models are averaged to get a model for each iteration. This average might slow down the convergence speed, although we acquire the ability to process large data set on multiple machines. So this algorithm provides an option <em>parallel</em> to let the user choose whether to do parallel computation.</p>
<p><b>Stopping Criteria</b> Both optimizers compute the average difference between the coefficients of two consecutive iterations, and if the difference is smaller than <em>tolerance</em> or the iteration number is larger than <em>max_iter</em>, the computation stops.</p>
<p><b>Online Help</b> The user can read short help messages by using any one of the following </p>
<div class="fragment"><pre class="fragment">SELECT madlib.elastic_net_train();
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;usage&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;predict&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;gaussian&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;binomial&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;linear&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;fista&#39;</span>);
SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;igd&#39;</span>);
</pre></div><dl class="user"><dt><b>Input:</b></dt><dd></dd></dl>
<p>The <b>training examples</b> is expected to be of the following form: </p>
<pre>{TABLE|VIEW} <em>input_table</em> (
    ...
    <em>independentVariables</em>   DOUBLE PRECISION[],
    <em>dependentVariable</em>      DOUBLE PRECISION,
    ...
)</pre><p>Null values are not expected.</p>
<dl class="user"><dt><b>Usage:</b></dt><dd></dd></dl>
<p><b>Pre-run :</b> Usually one gets better results and faster convergence using <em>standardize = True</em>. <b>It is highly recommended to run <em>elastic_net_train</em> function on a subset of the data with limited <em>max_iter</em> before applying it onto the full data set with a large <em>max_iter</em>. In the pre-run, the user can tweak the parameters to get the best performance and then apply the best set of parameters onto the whole data set.</b></p>
<ul>
<li>Get the fitting coefficients for a linear model:</li>
</ul>
<pre>
       SELECT {schema_madlib}.elastic_net_train (
            'tbl_source',     -- Data table
            'tbl_result',     -- Result table
            'col_dep_var',    -- Dependent variable, can be an expression or
                                    '*'
            'col_ind_var',    -- Independent variable, can be an expression
            'regress_family', -- 'gaussian' (or 'linear'). 'binomial'
                                    (or 'logistic') will be supported
            alpha,            -- Elastic net control parameter, value in [0, 1]   
            lambda_value,     -- Regularization parameter, positive
            standardize,      -- Whether to normalize the data. Default: True
            'grouping_col',   -- Group by which columns. Default: NULL
            'optimizer',      -- Name of optimizer. Default: 'fista'
            'optimizer_params',-- Optimizer parameters, delimited by comma. Default: NULL
            'excluded',       -- Column names excluded from '*'. Default: NULL
            max_iter,         -- Maximum iteration number. Default: 10000
            tolerance         -- Stopping criteria. Default: 1e-6
        );
</pre><p>If <em>col_ind_var</em> is '*', then all columns of <em>tbl_source</em> will be used as features except those listed in the <em>excluded</em> string. If the dependent variable is a column name, it is then automatically excluded from the features. However, if the dependent variable is a valid Postgres expression, then the column names inside this expression are not excluded unless explicitly put into the <em>excluded</em> list. So it is a good idea to put all column names involved in the dependent variable expression into the <em>excluded</em> string.</p>
<p>The <em>excluded</em> string is a list of column names excluded from features delimited by comma. For example, 'col1, col2'. If it is NULL or an empty string '', no column is excluded.</p>
<p>If <em>col_ind_var</em> is a single column name, which is the array type, one can still use <em>excluded</em>. For example, if <em>x</em> is a column name, which is an array of size 1000, and the user wants to exclude the 100-th, 200-th and 301-th elements of the array, he can set <em>excluded</em> to be '100, 200, 301'.</p>
<p>Both <em>col_dep_var</em> and <em>col_ind_var</em> can be valid Postgres expression. For example, <em>col_dep_var = 'log(y+1)'</em>, and <em>col_ind_var = 'array[exp(x[1]), x[2], 1/(1+x[3])]'</em> etc. In the binomial case, one can set <em>col_dep_var = 'y &lt; 0'</em> etc.</p>
<p>Output: </p>
<pre> family | features | features_selected | coef_nonzero | coef_all | intercept | log_likelihood | standardize | iteration_run
  ------------------+------------+------------+------------+--------------+-------------+--------+--------+-----------
  ...
  </pre><p>where <em>log_likelihood</em> is just the negative value of the first equation above (up to a constant depending on the data set).</p>
<ul>
<li>Get the <b>prediction</b> on a data set using a linear model: <pre>
SELECT madlib.elastic_net_predict(
    '<em>regress_family</em>',  -- Response type, 'gaussian' ('linear') or 'binomial' ('logistic')
    <em>coefficients</em>,    -- fitting coefficients
    <em>intercept</em>,  -- fitting intercept
    <em>independent Variables</em> 
) from tbl_data, tbl_train_result;
</pre> The above function returns a double value for each data point. When predicting with binomial models, the return value is 1 if the predicted result is True, and 0 if the prediction is False.</li>
</ul>
<p><b>Or</b> (1) </p>
<pre>
SELECT madlib.elastic_net_gaussian_predict (                 
    coefficients, intercept, ind_var                         
) FROM tbl_result, tbl_new_source LIMIT 10;
</pre><p>(2) </p>
<pre>
SELECT madlib.elastic_net_binomial_predict (                 
    coefficients, intercept, ind_var                         
) FROM tbl_result, tbl_new_source LIMIT 10;
</pre><p>This returns 10 BOOLEAN values.</p>
<p>(3) </p>
<pre>
SELECT madlib.elastic_net_binomial_prob (                    
    coefficients, intercept, ind_var                         
) FROM tbl_result, tbl_new_source LIMIT 10;
</pre><p>This returns 10 probability values for True class.</p>
<p><b>Or</b> The user can use another prediction function which stores the prediction result in a table. This is usefule if the user wants to use elastic net together with general cross validation function. </p>
<pre>
SELECT madlib.elastic_net_predict(
    '<em>tbl_train_result</em>',
    '<em>tbl_data</em>',
    '<em>col_id</em>',  -- ID associated with each row
    '<em>tbl_predict</em>'  -- Prediction result
);
</pre><dl class="user"><dt><b>Examples:</b></dt><dd></dd></dl>
<ol type="1">
<li>Prepare an input table/view: <div class="fragment"><pre class="fragment">CREATE TABLE en_data (
    ind_var DOUBLE PRECISION[],
    dep_var DOUBLE PRECISION
);
</pre></div></li>
<li>Populate the input table with some data, which should be well-conditioned, e.g.: <div class="fragment"><pre class="fragment">mydb=# INSERT INTO lasso_data values ({1, 1}, 0.89);
mydb=# INSERT INTO lasso_data values ({0.67, -0.06}, 0.3);
...
mydb=# INSERT INTO lasso_data values ({0.15, -1.3}, -1.3);
</pre></div></li>
<li>learn coefficients, e.g.: <div class="fragment"><pre class="fragment">mydb=# SELECT madlib.elastic_net_train(<span class="stringliteral">&#39;en_data&#39;</span>, <span class="stringliteral">&#39;en_model&#39;</span>, <span class="stringliteral">&#39;ind_var&#39;</span>, <span class="stringliteral">&#39;dep_var&#39;</span>, 0.5, 0.1,
                                        True, <span class="stringliteral">&#39;linear&#39;</span>, <span class="stringliteral">&#39;igd&#39;</span>, <span class="stringliteral">&#39;stepsize = 0.1, warmup = t,</span>
<span class="stringliteral">                                        warmup_lambda_no=3, warmup_lambdas = [0.4, 0.3, 0.2, 0.1],</span>
<span class="stringliteral">                                        parallel=t&#39;</span>, <span class="charliteral">&#39;1&#39;</span>, 10000, 1e-6);
</pre></div> <div class="fragment"><pre class="fragment">mydb=# select madlib.elastic_net_predict(family, coef_all, intercept, ind_var)
mydb-# from en_data, en_model;
</pre></div></li>
</ol>
<dl class="user"><dt><b>Literature:</b></dt><dd></dd></dl>
<p>[1] Elastic net regularization. <a href="http://en.wikipedia.org/wiki/Elastic_net_regularization">http://en.wikipedia.org/wiki/Elastic_net_regularization</a></p>
<p>[2] Beck, A. and M. Teboulle (2009), A fast iterative shrinkage-thresholding algorothm for linear inverse problems. SIAM J. on Imaging Sciences 2(1), 183-202.</p>
<p>[3] Shai Shalev-Shwartz and Ambuj Tewari, Stochastic Methods for l1 Regularized Loss Minimization. Proceedings of the 26th International Conference on Machine Learning, Montreal, Canada, 2009.</p>
<dl class="see"><dt><b>See also:</b></dt><dd>File <a class="el" href="elastic__net_8sql__in.html" title="SQL functions for elastic net regularization.">elastic_net.sql_in</a> documenting the SQL functions. </dd></dl>
</div>
</div>
  <div id="nav-path" class="navpath">
    <ul>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Functions</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>


    <li class="footer">Generated on Fri May 10 2013 01:37:13 for MADlib by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.5.1 </li>
   </ul>
 </div>


</body>
</html>
